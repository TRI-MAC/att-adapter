<html>
<head>
  <meta charset="UTF-8">
  <title>Att-Adapter: A Robust and Precise  Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder</title>
  <link href="dreambooth_style/style.css" rel="stylesheet">
</head>
<body>
  <div class="content">
    <h1>Att-Adapter: A Robust and Precise  Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder</h1>
    <p id="authors">
      Wonwoong Cho<sup>1</sup>,
      Yan-ying Chen<sup>2</sup>,
      Matt Klenk<sup>2</sup>,
      David Inouye<sup>1</sup>,
      Yanxia Zhang<sup>2</sup>
      <br>
      <br>
      <span style="font-size: 20px;"><sup>1</sup>Purdue University</span><br>
      <span style="font-size: 20px;"><sup>2</sup>Toyota Research Institute</span>
    </p>
    <p style="text-align:center;">
      <a href="https://arxiv.org/pdf/2503.11937" target="_blank">[Paper]</a>
      &nbsp;&nbsp;&nbsp;
      <a href="https://github.com/WonwoongCho/Att-Adapter" target="_blank">[Code]</a>
      &nbsp;&nbsp;&nbsp;
      <a href="#bibtex" target="_blank">[BibTeX]</a>
      &nbsp;&nbsp;&nbsp;
      <a href="https://huggingface.co/WonwoongCho/Att-Adapter/tree/main" target="_blank">[Pretrained Models]</a>
    </p>
  </div>

  <div class="content">
    <h2>Abstract</h2>
    <p>
      While text-to-image (T2I) diffusion models can generate high-quality images from text prompts, they struggle to provide precise, continuous control over specific visual attributes (e.g., nose width, eye openness), especially in new domains. Existing methods often require paired data, handle only discrete attributes, or depend on pretrained GANs.
    </p>
    <p>
      We introduce <b>Att-Adapter</b>, a plug-and-play module for pretrained diffusion models that enables robust, fine-grained, multi-attribute control from unpaired data. By leveraging a decoupled cross-attention module and a Conditional Variational Autoencoder (CVAE), Att-Adapter harmonizes multiple domain attributes with text and mitigates overfitting. Our approach supports continuous and indescribable attribute manipulation, requires no paired synthetic data, and scales to many attributes in a single model.
    </p>
    <img src="images/motivating example 1.jpg" alt="Motivating Example: Attribute control" style="width:100%;max-width:700px;display:block;margin:auto;">
    <p style="text-align:center;"><em>Att-Adapter enables continuous control (e.g., 0 to 1) over subtle visual details like nose width or eye openness, which are difficult to capture with discrete text-based conditioning.</em></p>
  </div>

  <div class="content">
    <h2>Approach and Context</h2>
    <p>
      Large pretrained text-to-image (T2I) diffusion models have achieved impressive results in generating realistic images from text prompts. However, providing users with precise, fine-grained control over multiple, continuous, or indescribable visual attributes—such as nose width or eye openness—remains a significant challenge, especially in new domains. Existing approaches often rely on text-based conditioning, which is inherently limited for subtle or continuous attributes, or require paired data and pretrained GANs, restricting their flexibility and applicability.
    </p>
    <p>
      Recent methods have attempted to address these limitations, but many either struggle with indescribable attributes, depend on synthetic paired data, or require training separate models for each attribute. For example, approaches based on CLIP embeddings or StyleGAN latent spaces are limited by their reliance on text or GAN quality, while LoRA-based controllers often need paired data and lack scalability to multi-attribute control.
    </p>
    <p>
      <b>Att-Adapter</b> overcomes these challenges by introducing a plug-and-play module for pretrained diffusion models that enables robust, fine-grained, and multi-attribute control from unpaired real data. Our approach leverages a decoupled cross-attention module to harmonize multiple domain attributes with text conditioning, and incorporates a Conditional Variational Autoencoder (CVAE) to mitigate overfitting and model the diversity of real-world attributes. This design allows Att-Adapter to support both continuous and discrete attribute spaces, handle indescribable attributes, and scale to many attributes within a single model—without requiring paired synthetic data or separate models for each attribute.
    </p>
    <img src="images/method_overview.jpg" alt="Att-Adapter Overview" style="width:100%;max-width:1000px;display:block;margin:auto;">
    <h3 style="text-align:center;"><em>Overview of the Att-Adapter approach for attribute control in diffusion models.</em></h3>
  </div>

  <div class="content">
    <h2>Results</h2>
    <style>
      .results-figure {
        width: 100%;
        max-width: 900px;
        display: block;
        margin: 32px auto 8px auto;
        border-radius: 10px;
        box-shadow: 0 2px 16px rgba(0,0,0,0.08);
        background: #fff;
        padding: 8px;
      }
      .results-caption {
        text-align: center;
        font-size: 1.05em;
        margin-bottom: 24px;
        color: #444;
      }
    </style>
    <p>
      We evaluate Att-Adapter on two public datasets (FFHQ for faces and EVOX for cars), comparing against strong baselines for both continuous and discrete attribute control. Our experiments cover both latent and absolute attribute control settings, and we report both quantitative and qualitative results.
    </p>
    <p>
      <b>Latent Control:</b> Att-Adapter outperforms baselines such as ConceptSlider&nbsp;<a href="#ref-gandikota2024concept" id="ref-gandikota2024concept-ref">[a]</a>, <span style="font-family:monospace;">W<sub>+</sub></span> Adapter&nbsp;<a href="#ref-li2024stylegan" id="ref-li2024stylegan-ref">[b]</a>, and S-I-GANs&nbsp;<a href="#ref-shen2020interfacegan" id="ref-shen2020interfacegan-ref">[c]</a> in both control range (CR) and disentanglement (DIS) for single and multiple attribute settings. Our method achieves higher expressivity and better disentanglement, as shown in the table below and the qualitative comparison.
    </p>
    <img class="results-figure" src="images/continuous_baseline_comparison.jpg" alt="Continuous Attribute Comparison">
    <div class="results-caption"><em>Comparison of continuous attribute control with baselines. Att-Adapter achieves stronger target attribute effects and better disentanglement.</em></div>
    <p>
      <b>Absolute Control:</b> In the absolute control setting, Att-Adapter demonstrates superior performance compared to LoRA and ITI-GEN, especially in controlling fine-grained facial attributes and extrapolating beyond the training range. Our method maintains high visual fidelity and better alignment with target attributes.
    </p>
    <img class="results-figure" src="images/extrapolation.jpg" alt="Extrapolation Results">
    <div class="results-caption"><em>Extrapolation to unseen attribute ranges. Att-Adapter can generalize beyond the training domain, unlike token-based baselines.</em></div>
    <p>
      <b>Generalization to New Domains:</b> On the EVOX car dataset, Att-Adapter provides robust control over both discrete (e.g., pose, body type) and continuous (e.g., car size) attributes, outperforming LoRA in continuous attribute manipulation and maintaining pretrained knowledge.
    </p>
    <img class="results-figure" src="images/evox.jpg" alt="EVOX Car Attribute Control">
    <div class="results-caption"><em>Comparison of attribute control on the EVOX car dataset. Att-Adapter achieves better control over continuous attributes and maintains prompt fidelity.</em></div>
    <p>
      <b>Compositionality and Adapter Fusion:</b> Att-Adapter can be combined with other adapters, such as IP-Adapter, to enable image-based conditioning alongside attribute control. This demonstrates the flexibility and compositionality of our approach.
    </p>
    <img class="results-figure" src="images/ipdapter_examples.jpg" alt="IP-Adapter Fusion Example">
    <div class="results-caption"><em>Combining Att-Adapter with IP-Adapter enables both image and attribute-based conditioning in pretrained diffusion models.</em></div>
    <p>
      <b>Multi-Attribute Control:</b> Att-Adapter enables simultaneous and precise control over multiple visual attributes, demonstrating robust disentanglement and flexibility. The example below shows how Att-Adapter can manipulate several attributes at once, maintaining high image quality and attribute fidelity.
    </p>
    <img class="results-figure" src="images/multi_attributes_main.jpg" alt="Multi-Attribute Control Example">
    <div class="results-caption"><em>Att-Adapter supports fine-grained, multi-attribute control, allowing for complex and realistic edits across several attributes simultaneously.</em></div>
  </div>

  <div class="content">
    <h2>Conclusion</h2>
    <p>
      Att-Adapter enables fine-grained, multi-attribute control in diffusion models, outperforming existing baselines and supporting both continuous and discrete domains. Our approach opens new possibilities for controllable image synthesis.
    </p>
  </div>

  <div class="content" id="acknowledgements">
    <h2>Acknowledgements</h2>
    <p>
      We thank all collaborators and contributors to this project. For more details, please see our <a href="https://arxiv.org/pdf/2503.11937" target="_blank">paper</a>.
    </p>
    <div id="references" style="font-size: 0.95em; margin-top: 2em;">
      <b>References</b><br>
      <span id="ref-gandikota2024concept"><b>[a]</b> Gandikota, V., et al. "ConceptSlider: Text-Guided Editing of Indescribable Attributes." CVPR 2024.</span><br>
      <span id="ref-li2024stylegan"><b>[b]</b> Li, Y., et al. "StyleGAN-based $W_+$ Adapter for Attribute Control." CVPR 2024.</span><br>
      <span id="ref-shen2020interfacegan"><b>[c]</b> Shen, Y., et al. "InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs." CVPR 2020.</span>
    </div>
    <div id="bibtex" style="font-size: 0.95em; margin-top: 2em;">
      <b>BibTeX</b>
      <pre>
@inproceedings{attadapter2025,
  title={Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder},
  author={Wonwoong Cho and Yan-ying Chen and Matt Klenk and David Inouye and Yanxia Zhang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2025},
  note={To appear}
}
      </pre>
    </div>
  </div>
</body>
</html>
